Part of the problem is a bottleneck at the heart of traditional sensors, which capture a huge amount of visual data, 
regardless of whether or not it is useful for classifying an image. Crunching all that data slows things down.

--------------------http://szeliski.org/Book/

A crash course on NumPy for images (skimages)
https://scikit-image.org/docs/dev/user_guide/numpy_images.html


--------------------------------------------------------------------------------------------

-In Image processing, *blob detection* refers to modules that are aimed at detecting points and/or regions in the image 
that differ in properties like 
brightness or color compared to the surrounding.
... It is used to obtain regions of interest for further processing.

-Pyramid, or pyramid representation, is a type of multi-scale signal representation developed by the computer vision, 
image processing and signal processing communities, in which a signal or an image is subject to 
repeated smoothing and subsampling. 
Pyramid representation is a predecessor to scale-space representation and multiresolution analysis. 

Stereo matching is the process of taking two or more images and estimating a 3D model of the scene 
by finding matching pixels in the images and converting their 2D positions into 3D depths.



-"For many applications of image processing, color information doesn't help us
identify important edges or other features." exceptions apply (for faces, colour usually is a good indicator, apparently).

-Thresholding 
(separating between foreground and background) is the simplest method of image segmentation.
+ You should use local thresholding instead of global if the image has a wide variation of background intensity.
+As we saw in the video, not being sure about what thresholding method to use isn't a problem.
In fact, scikit-image provides us with a function (from skimage.filters import try_all_threshold)
to check multiple methods and see 
for ourselves what the best option is. 
It returns a figure comparing the outputs of different global thresholding methods.

-A Gaussian filter can blur (reduce sharpness) an image to reduce noise and improve the likelihood of detecting something specific 
(e.g. edges, pero no creo que aplique a todo).

-Through the following code, you calculate the range of the pixels intensities in the histogram,
and so, the contrast of the image!

In [1]: np.min(clock_image)
Out[1]: 99

In [2]: np.max(clock_image)
Out[2]: 247

247-99=148 is the contrast

-# Use histogram equalization to improve the contrast
         xray_image_eq =  exposure.equalize_hist(chest_xray_image)
         
-"Amazing! You have increased the contrast of the image using an algorithm for local contrast enhancement,
that uses histograms computed over different tile regions of the image.
Local details can therefore be enhanced even in regions that are darker or lighter than the rest of the image."

-"Remember that aliasing is an effect that causes different signals, in this case pixels,
to become indistinguishable or distorted." (and hence anti-aliasing hace que no se distorsionen si las resize/rescale)
+anti aliasing preserves the quality (to an extent) but loses sharpness)

# Rescale with anti aliasing
rescaled_with_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=True, multichannel=True) multichannel means is a colour pic

-2 morphological operations"  (them both are useful to remove noise[among other benefits])
Dilation: adds pixels from the boundaries of objetcs
Erosion: removes pixels from the boundaries of objects
(pro tip, erosion is useful for removing minor white noise.)


-Inpainting 
is the process of reconstructing lost or deteriorated parts of images and videos.

-Denoising filters (total variation vs bilateral)
Since we prefer to preserve the edges in the image, we'll use the bilateral denoising filter.
total variation reduces noise by reducing variance

-The total amount of pixel is its resolution. Given by Height×Width.

Use .shape from NumPy to check the width and height of the image.

In [1]: np.shape(face_image)
Out[1]: (265, 191, 3)

face_image is 265 * 191 = 50,615 pixels


- Superpixel Segmentation (create pixel regions on the image).
You reduced the image from 50,615 pixels to 400 regions! 
Much more computationally efficient for, for example, face detection ML models.
(how much quality, important info, do we lose?)

# Obtain the segmentation with 400 regions
segments = slic(face_image, n_segments= 400)

# Put segments ON TOP of original image to compare
segmented_image = label2rgb(segments,face_image, kind='avg')  tienes que poner primero segmnets y luego face_image

- A contour 
is an outline formed by multiple points joined together.


-Representing an image by its edges 
has the advantage that the amount of data is reduce significantly while retaining most of the image information,
like the shapes. (((mmmmm most?)

-"Canny"== standard edge detection method

If we want to detect less edges (say, just the outer edge of a coin and not the inside)
we can increase the Gaussian filter to add more noise and detect less edges.
In the canny function, this can be accessed by the sigma= 

(the bigger the sigma value [Gaussian filter], the less edges are detected)

-Harris corner detection (popoular corner detection method)

-Make processes more computationally efficient with 
unsupervised superpixel segmentation. 


-The matplotlib function imshow() 
creates an image from a 2-dimensional numpy array. 
The image will have one square for each element of the array. 
The color of each square is determined by the value of the corresponding array element and the color map used by imshow() 


-Video content analysis (also video content analytics, VCA)
is the capability of automatically analyzing video to detect and determine temporal and spatial events.

-"What is a watermark?

Originally a watermark is a more or less transparent image or text that has been applied to a piece of paper,
another image to either protect the original image, or to make it harder to copy the item 
e.g. money watermarks or stamp watermarks.

-"
Inpainting is the process of ---reconstructing lost or deteriorated parts of images and videos---. 
In the museum world, in the case of a valuable painting, this task would be carried out by a skilled art conservator
or art restorer. In the digital world,
inpainting refers to the application of sophisticated algorithms
to replace lost or corrupted parts of the image data

-The Fourier transform (FT) decomposes a function (often a function of time, or a signal) into its constituent frequencies. 
A special case is the expression of a musical chord in terms of the volumes and frequencies of its constituent notes. 
The term Fourier transform refers to both the frequency domain representation
and the mathematical operation that 
associates the frequency domain representation to a function of time. 

- Point spread function

"
Assume we have a ‘point’ source, or a source that is far smaller than the maximum resolution (a pixel).
When we take an image of it, it will ‘spread’ over an area. 
To quantify that spread, we can define a ‘function’. 
This is how the point spread function or the PSF of an image is defined. 

This ‘spread’ can have various causes, for example in ground based astronomy, due to the atmosphere.
In practice we can never surpass the ‘spread’ due to the diffraction of the lens aperture.
Various other effects can also be quantified through a PSF. 
For example, the simple fact that we are sampling in a discrete space, namely the pixels, 
also produces a very small ‘spread’ in the image.

Convolution is the mathematical process by which we can apply a ‘spread’ to an image,
or in other words blur the image, see Convolution process. 
The Brightness of an object should remain unchanged after convolution, see Flux Brightness and magnitude. 
Therefore, it is important that the sum of all the pixels of the PSF be unity. 
The PSF image also has to have an odd number of pixels on its sides so one pixel can be defined as the center.


-Shape completion, the problem of --estimating the complete geometry of objects from partial observations--,
lies at the core of many vision and robotics applications.
In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. 


-A photo’s EXIF data contains a ton of information about your camera,
and potentially where the picture was taken (GPS coordinates). 
That means, if you’re sharing images, there’s a lot of details others can glean from them.

EXIF stands for Exchangeable Image File Format.
Every time you take a picture with your digital camera or phone,
a file (typically a JPEG) is written to your device’s storage. 
In addition to all the bits dedicated to the actual picture, it records a considerable amount of 
supplemental metadata as well. This can include date, time, camera settings, and possible copyright information. 
You can also add further metadata to EXIF, such as through photo processing software.




"---->Preserving Detail in Image Inputs

Given real-world constraints on memory and processing time, images are often downsampled before they’re fed 
into a neural network. 
But the process removes fine details, and that degrades accuracy. 
A new technique squeezes images with less compromise.

What’s new: 
Researchers at the Alibaba DAMO Academy and Arizona State University led by Kai Xu
--reduce the memory needed for image processing --by using a technique inspired by JPEG image compression.

Key insight: 
JPEG removes information the human eye won’t miss by describing patterns of pixels as frequencies. 
Successive color changes from pixel to pixel are higher frequencies, while monochromatic stretches are lower frequencies. 
---By cutting frequencies that have little visual effect, ---
the algorithm compresses images with minimal impact on image quality.
The researchers employed a similar strategy to reduce input data without losing information critical to learning.

How it works: The researchers transformed images into the frequency domain, 
selected frequencies to remove, and fed the reduced frequency representation into ResNet-50 and MobileNet V2 models.





Research in Sensor Processing (1960’s and 1970's)

"Dr. Hubel and Dr. Wiesel worked on the area of Sensory Processing. 
In which, they inserted a micro-electrode into the primary visual cortex of an partially 
anesthetized cat so that she can’t move and shown the images of line at different angles to the cat.



--

Resolution. 

Image resolution is typically described in PPI,
which refers to how many pixels are displayed per inch of an image. 

Higher resolutions mean that there more pixels per inch (PPI), 
resulting in more pixel information and creating a high-quality, crisp image.

--

>>>>>Lossy Compression

Lossy compression involves eliminating some of the data in your image.
Because of this, it means you might see degradation (reduction in quality or what some refer to as pixelated).

So you have to be careful by how much you’re reducing your image.
Not only due to quality, but also because you can’t reverse the process.

Of course, one of the great benefits of lossy compression and why it’s one of the most popular compression methods 
is that you can reduce the file size by a very large amount.

    JPEGs and GIFs are both lossy image formats.
    JPEGs are great for sites needing fast load times as you can adjust the quality level for a good balance of quality and file size.
    
    

>>>>>Lossless Compression

Lossless compression, unlike lossy, doesn’t reduce the quality of the image. 
How is this possible? It’s usually done by removing unnecessary metadata (automatically generated data produced by the device capturing the image). 

However, the biggest drawback to this method is that you won’t see a significant reduction in file size. 
In other words, it will take up a lot of disk space over time.

    RAW, BMP, GIF, and PNG are lossless image formats.
    You can perform a lossless compression on your desktop using tools such as Photoshop, FileOptimizer, or ImageOptim.
    Some plugins will apply Gzip compression to images (minify them).

If we do a little comparison of lossy compression rates, 
we can see that when using lossless compression you don’t lose any quality whatsoever. 
However, the file size of the image was only reduced by 10.84%. This is compared to over 90% when using lossy compression.


--

Signal-to-noise ratio (abbreviated SNR or S/N)

is a measure used in science and engineering that 
>>compares the level of a desired signal to the level of background noise. 

SNR is defined as the ratio of signal power to the noise power, often expressed in decibels.

A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise. 


--

EXIF data

Such stored data is called “EXIF Data”, and it is comprised of a range of settings such as
ISO speed, shutter speed, aperture, white balance, camera model and make, date and time, lens used, focal length, and much more.

Being able to read such data can be of great importance not only for beginners but also for other photographers who want to find out 
what settings and tools were used to create a particular photograph.

Unfortunately, 
though, the only web-friendly (in terms of size) file format that can handle EXIF is JPEG, 
which means that you often cannot read the data from other image formats such as GIF and PNG.

In addition, some photographers choose to strip EXIF data from their images to protect their work and their style, 
while others do it to save website traffic (yes, EXIF does add up to the size of the file). 

Those, who leave this data in their images either have no idea that they even have it, 
or they intentionally leave it like I do – for others to see and possibly learn from.

-----

Image GPT

https://openai.com/blog/image-gpt/

From language GPT to image GPT

In language, unsupervised learning algorithms that rely on word prediction (like GPT-2 and BERT) have been extremely successful, 
achieving top performance on a wide array of language tasks.
One possible reason for this success is that instances of downstream language tasks appear naturally in text: 
questions are often followed by answers (which could help with question-answering)
and passages are often followed by summaries (which could help with summarization). 

In contrast, sequences of pixels do not clearly contain labels for the images they belong to.

Even without this explicit supervision, 
there is still a reason why GPT-2 on images might work: a sufficiently large transformer trained on next pixel prediction 
might eventually learn to generate diverse
samples with clearly recognizable objects. 

Once it learns to do so, an idea known as 
---->“Analysis by Synthesis”
suggests that the model will also know about object categories. 

Many early generative modelswere motivated by this idea, and more recently, 
BigBiGAN37 was an example which produced encouraging samples and features. 

In our work, we first show that better generative models achieve stronger classification performance. 
Then, through optimizing GPT-2 for generative capabilities, we achieve top-level classification performance in many settings, 
providing further evidence for analysis by synthesis.



-----
What are Fourier transforms used for?

The Fourier Transform is an important image processing tool 
which is used to decompose an image into its sine and cosine components. 

The output of the transformation represents the image in the Fourier or frequency domain,
while the input image is the spatial domain equivalent.

------

Learning Physical Graph Representations from Visual Scenes

https://mitibmwatsonailab.mit.edu/research/blog/learning-physical-graph-representations-from-visual-scenes/

Convolutional Neural Networks (CNNs) 
have proved exceptional at learning representations for visual object categorization. 

However, CNNs do not explicitly encode objects, parts, and their physical properties, 
which has limited CNNs’ success on tasks that require structured understanding of visual scenes. 

To overcome these limitations, we introduce the idea of --Physical Scene Graphs (PSGs)--, 
>>>>which represent scenes as --hierarchical graphs--, 
with nodes in the hierarchy corresponding intuitively to object parts at different scales,
and edges to physical connections between parts. 

>>Bound to each node is a vector of latent attributes 
that intuitively represent object properties 
such as surface shape and texture. 

We also describe PSGNet, 
a network architecture that learns to extract PSGs 
by reconstructing scenes 
through a PSG-structured bottleneck. 

PSGNet augments standard CNNs by including: 
recurrent feedback connections to combine low and high-level image information; 
graph pooling and vectorization operations 
that convert spatially-uniform feature maps into object-centric graph structures;
and perceptual grouping principles 
to encourage the identification of meaningful scene elements. 

We show that PSGNet outperforms alternative self-supervised scene representation algorithms
at scene segmentation tasks, 
especially on complex real-world images, 
and generalizes well to unseen object types and scene arrangements. 

PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. 
We present a series of ablation studies illustrating the importance of each component of the PSGNet architecture, 
analyses showing that 
>>>> learned latent attributes capture intuitive scene properties, 
and illustrate the use of PSGs for compositional scene inference.

--------

Image-to-image translation
is a class of vision and graphics problems where the goal is to 
learn the mapping 
between an input image and an output image. 


------

from the batch 

Transformer Speed-Up Sped Up

The transformer architecture is notoriously inefficient when processing long sequences — a problem in processing 
images, which are essentially long sequences of pixels.

One way around this is to break up input images and process the pieces separately. New work improves upon this already-streamlined approach.

What’s new: Zizhao Zhang and colleagues at Google and Rutgers University simplified an earlier proposal for using transformers to process images. 
They call their architecture NesT.

Key Insight:  A transformer that processes parts of an image and then joins them can work more efficiently than
one that looks at the entire image at once. However, to relate the parts to the whole, it must learn how the pixels 
in different regions relate to one another. 
A recent model called Swin does this by shifting region boundaries in between processing regions and merging them together 
— a step that nonetheless consumes compute cycles.
Using convolutions to process both within and across regions can enable a model to learn such relationships
without shifting region boundaries, saving that computation.












