
- andrew ng "Better Than Backprop

End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. 
However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. 
What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM),
an unsupervised method for learning to extract features that trains only one layer at a time.

Key insight: The information bottleneck theory (IB) suggests that neural networks work by
concentrating information like a data-compression algorithm. In data compression,
the amount of information retained is measured in mutual information (MI) between original and compressed versions. 
IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning
as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.


    GIM uses the previous layer’s output as the next layer’s input to train each layer independently. 
    This differs from the usual backpropagation in which all layers learn at once.
    
    The researchers devised a task that teaches layers to extract features that maximize MI.
    Given a subsequence of input data that has been compressed according to the current weights, 
    the layer predicts the next element in the compressed sequence, choosing from a random selection drawn 
    from the input including the correct choice. High success demonstrates that the layer is able to compress the input.
    
    The process effectively removes redundancy between nearby regions of the input. 
    For example, a recording of a song’s chorus may repeat several times, 
    so it’s possible to represent the recording without capturing the repetitions.
    
    
    --- MLPerf
    
        Fair and useful benchmarks for measuring -training and inference- performance of ML hardware, software, and services.

 
 

----
"" New software can now fool the AI behind Alexa and Siri.

Software called TextFooler can now trick natural-language processing (NLP) systems into misunderstanding text 
just by replacing certain words in a sentence with synonyms. 
Developed by a team at MIT, it looks for the words that are most important to an NLP classifier
and replaces them with a synonym that a human would find natural. 
For example, it changes the sentence “The characters, cast in impossibly contrived situations, 
are totally estranged from reality” to “The characters, cast in impossibly engineered circumstances,
are fully estranged from reality.”
We see no difference, but AI interprets the sentences completely differently.


-----
"" Medication madness. According to the FDA, serious adverse drug interactions are estimated to kill 
more than 100,000 hospitalized people in the US every year.
But avoiding such interactions during drug development is laborious and expensive. 
It involves intensive testing and clinical trials to catalogue all the proposed drug’s possible chemical interactions
with existing ones.

Now a new AI system could make this easier by predicting the interactions
between two drugs based only on their chemical structure. 
The researchers first developed a new way to represent the 3D chemical structures of drugs in a character format
that could be read by a neural network. The drug melatonin, for example, is represented by “CC(=O)NCCC1=CNc2c1cc(OC)cc2.”

They then translated a database of known drug interactions into this format
and used it to train a neural network. The resulting system predicts the probability 
that two drugs will have an adverse interaction and shows the particular parts of the molecule 
that contributed to that prediction. When the researchers tested their system on two common drug interaction data sets,
it performed better than state-of-the-art results. Read more here.


---" Quotable

It’s really snowballed.

—Vincent Cate, who lives on the island of Anguilla, 
which has become an unlikely benefactor of the AI boom because of its domain name “.ai”


---Double descent ---
occurs when a model’s performance changes in unpredictable ways
as the amount of training data or number of parameters crosses a certain threshold. 
The error falls as expected with additional data or parameters, but then rises, drops again, and may take further turns. 
Preetum Nakkiran and
collaborators at Harvard, Stanford, and Microsoft found a way to eliminate double descent in some circumstances.


-
Probabilistic Programming Languages

https://towardsdatascience.com/a-gentle-introduction-to-probabilistic-programming-languages-ba9105d9cbce

"Conceptually, probabilistic programming languages(PPLs) are domain-specific languages
that describe probabilistic models 
and the mechanics to perform inference in those models.
The magic of PPL relies on combining the inference capabilities of probabilistic methods 
with the representational power of programming languages.





--
One-shot learning

https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

 There are applications wherein we neither have enough data for each class and the total number classes is huge 
 as well as dynamically changing. 
 Thus, the cost of data collection and periodical re-training is -->  too high.
 
 


--
Siamese networks
https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

Typically the similarity score is squished between 0 and 1 using a sigmoid function;
wherein 0 denotes no similarity and 1 denotes full similarity. 
Any number between 0 and 1 is interpreted accordingly.

Notice that this network is not learning to classify an image directly to any of the output classes.
Rather, it is learning a similarity function,
which takes two images as input and expresses how similar they are.

Oneshot
But the biggest advantage is that , let’s say in case of face recognition,
we have a new employee who has joined the organization. Now in order for the network to detect his face, 
we only require a single image of his face which will be stored in the database.
Using this as the reference image,
the network will calculate the similarity for any new instance presented to it.
Thus we say that network predicts the score in one shot.

Notice that we will train the system on one set of characters 
and then test it on a completely different set of characters
which were never used during the training. 
This is not possible in a traditional classification cycle.

Mapping the problem to binary classification task
Let’s understand how can we map this problem into a supervised learning task 
where our dataset contains pairs of (Xi, Yi) 
where ‘Xi’ is the input and ‘Yi’ is the output.

Recall that the input to our system will be a pair of images
and the output will be a similarity score between 0 and 1.

Xi = Pair of images

Yi = 1 ; if both images contain the same character

Yi = 0; if both images contain different characters

Thus we need to create pairs of images along with the target variable, as shown above, 
to be fed as input to the Siamese Network.
Note that even though characters from Sanskrit alphabet are shown above, 
but in practice we will generate pairs randomly from all the alphabets in the training data.

Intuition: The term Siamese means twins. 
The two Convolutional Neural Networks shown above are not different networks
but are two copies of the same network, hence the name Siamese Networks.
Basically they share the same parameters. 
The two input images (x1 and x2) are passed through the ConvNet 
to generate a fixed length feature vector for each (h(x1) 
and h(x2)). 
Assuming the neural network model is trained properly, we can make the following hypothesis: 
If the two input images belong to the same character, then their >>feature vectors must also be similar,
while if the two input images belong to the different characters, then their feature vectors will also be different.
Thus the element-wise absolute difference 
between the two feature vectors 
must be very different in both the above cases. 
And hence the similarity score generated by the output sigmoid layer must also be different in these two cases.
This is the central idea behind the Siamese Networks.

Notice that there is no predefined layer in Keras to compute the absolute difference between two tensors.
We do this using the Lambda layer in Keras which is used to add customized layers in Keras.
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])


The model was compiled using the adam optimizer and binary cross entropy loss function.
Learning rate was kept low as it was found that with high learning rate, the model took a lot of time to converge. 
However these parameters can well be tuned further to improve the present settings.

The model was trained for 20,000 iterations with batch size of 32.
After every 200 iterations, model validation was done using 20-way one shot learning 
and the accuracy was calculated over 250 trials.

VAlidate and test
Note that, for every pair of input images, 
our model generates a similarity score between 0 and 1. 
But just looking at the score its difficult to ascertain whether the model is really able to recognize
similar characters and distinguish dissimilar ones.
A nice way to judge the model is >>>>> N-way one shot learning.

Basically the same character is compared to 4 different characters 
out of which only one of them matches the original character. 
Let’s say by doing the above 4 comparisons we get 4 similarity scores S1, S2, S3 and S4 as shown.
Now if the model is trained properly, we expect that S1 is the maximum of all the 4 similarity scores
because the first pair of images is the only one where we have two same characters.

Thus if S1 happens to be the maximum score,
we treat this as a correct prediction otherwise we consider this as an incorrect prediction.
Repeating this procedure ‘k’ times, we can calculate the percentage of correct predictions as follows.


Base Line 1 — Nearest Neighbor Model
It is always a good practice to create simple baseline models and compare their results with the complex model 
you are trying to build.

Conclusion
This is just a first cut solution and many of the hyper parameters can be tuned >>in order to avoid over fitting.
Also more rigorous testing can be done by increasing the value of ’N’ 
in N-way testing and by increasing the number of trials.

--
tf.keras.layers.Lambda
Wraps arbitrary expressions as a Layer object.

Inherits From: Layer


tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs)
    
    in siamese 
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    
    
    --
    Lambda functions
    
    Python and other languages like Java, C#, and even C++ have had lambda functions added to their syntax,
    whereas languages like LISP or the ML family of languages, Haskell, OCaml, and F#, use lambdas as a core concept.

>>Python lambdas are little, anonymous functions, 
subject to a more restrictive but more concise syntax than regular Python functions.

By the end of this article, you’ll know:

    How Python lambdas came to be
    How lambdas compare with regular function objects
    How to write lambda functions
    Which functions in the Python standard library leverage lambdas
    When to use or avoid Python lambda functions
    
    History

Alonzo Church formalized lambda calculus,
a language based on pure abstraction, in the 1930s. 
Lambda functions are also referred to as lambda abstractions, 
a direct reference to the abstraction model of Alonzo Church’s original creation.

Lambda calculus can encode any computation. 
It is Turing complete, but contrary to the concept of a Turing machine, it is pure and >>does not keep any state.

Functional languages
get their origin in mathematical logic and lambda calculus,
while imperative programming languages
embrace the state-based model of computation invented by Alan Turing.
The two models of computation, lambda calculus and Turing machines, can be translated into each another.
This equivalence is known as the Church-Turing hypothesis.

Functional languages directly inherit the lambda calculus philosophy,
adopting a declarative approach of programming that emphasizes abstraction,
data transformation, composition, and purity (no state and no side effects).
Examples of functional languages include Haskell, Lisp, or Erlang.

By contrast, the Turing Machine led to imperative programming found in languages like Fortran, C, or Python.

The imperative style consists of programming with statements,
driving the flow of the program step by step with detailed instructions.
This approach promotes mutation and requires managing state.

Python is not inherently a functional language,
but it adopted some functional concepts early on. 
In January 1994, map(), filter(), reduce(), and the lambda operator were added to the language.

The following terms may be used interchangeably depending on the programming language type and culture:

    Anonymous functions
    Lambda functions
    Lambda expressions
    Lambda abstractions
    Lambda form
    Function literals

For the rest of this article after this section, you’ll mostly see the term lambda function.

Taken literally, an anonymous function is a function without a name.
In Python, an anonymous function is created with the lambda keyword. 
More loosely, it may or not be assigned a name.
Consider a two-argument anonymous function defined with lambda but not bound to a variable.
The lambda is not given a name:

>>> lambda x, y: x + y

The function above defines a lambda expression that takes two arguments and returns their sum.
--

Deep Double Descent

OpenAI

https://openai.com/blog/deep-double-descent/

We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves,
then gets worse, and then improves again with increasing model size, data size, or training time. 
This effect is often avoided through careful regularization.
While this behavior appears to be fairly universal, we don’t yet fully understand why it happens,
and view further study of this phenomenon as an important research direction.

Moreover, we show that double descent occurs not just as a function of model size,
but also as a function of the number of training epochs. 

We unify the above phenomena by defining a new complexity measure 
we call the effective model complexity 
and conjecture a generalized double descent with respect to this measure. 

Furthermore, our notion of model complexity allows us to identify certain regimes 
where increasing (even quadrupling) the number of train samples 
actually hurts test performance.



---

Life long ML
Chen and Lei

Lifelong Machine Learning or Lifelong Learning (LL) is an advanced machine learning (ML) paradigm 
that learns continuously, accumulates the knowledge learned in the past,
and uses/adapts it to help future learning and problem solving. 
In the process, the learner becomes more and more knowledgeable and better and better at learning.



__
Semi supervised learning

FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence

Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel

    Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data
    to improve a model's performance.
    In this paper, we demonstrate the power of a simple combination of two common SSL methods:
    consistency regularization and pseudo-labeling.
    
    Our algorithm, FixMatch, first generates pseudo-labels
    using the model's predictions on weakly-augmented unlabeled images.
    
    >>For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction.
    The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.
    Despite its simplicity, we show that FixMatch achieves state-of-the-art performance 
    across a variety of standard semi-supervised learning benchmarks,
    including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class.
    
    Since FixMatch bears many similarities to existing SSL methods 
    that achieve worse performance, we carry out an extensive ablation study
    to tease apart the experimental factors that are most important to FixMatch's success.
    We make our code available at this https URL. 



---

Q Learning


Q-learning is an off policy reinforcement learning algorithm 
that seeks to find the best action to take given the current state. 

It’s considered off-policy because the q-learning function learns from actions that are outside the current policy,
like taking random actions, and therefore a policy isn’t needed. 

More specifically, q-learning seeks to learn a policy that maximizes the total reward.

The ‘q’ in q-learning stands for quality. 
Quality in this case represents how useful a given action is in gaining some future reward.

Create a q-table
When q-learning is performed we create what’s called a q-table or matrix 
that follows the shape of [state, action] and we initialize our values to zero.
We then update and store our q-values after an episode. 
This q-table becomes a reference table for our agent to select the best action based on the q-value.




--
Graph Deep Learning (GDL) 
Graph Neural Network (GNN)
https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783

The black arrows on the edges represent the kind of relationship between the nodes.
It shows whether a relationship is mutual or one-sided.
The two different kinds of graphs are 
---directed (connection direction matters between nodes)
----- undirected (connection order doesn’t matter). 
Directed graphs can be unidirectional or bidirectional in nature.

A graph can represent many things — social media networks, molecules, etc.
Nodes can be thought of as users/products/atoms 
while the edges represent connections (following/usually-purchased-with/bonds). 
A social media graph may look like this with nodes as users and edges as connections

Each node has a set of features defining it. 
In the case of social network graphs, this could be age, gender, country of residence, political leaning, and so on. 
Each edge may connect nodes together that have similar features. (yo/ necessarily similar? nah, otras relaciones tmb)
It shows some kind of interaction or relationship between them.

Message Passing
Once the conversion of nodes and edges are completed,
the graph performs Message Passing between the nodes. 
This process is also called Neighbourhood Aggregation 
because it involves pushing messages (aka, the embeddings) 
from surrounding nodes around a given reference node, through the directed edges. (feed forward nns)

Once you perform the Neighbourhood Aggregation/Message Passing procedure a few times,
you obtain a completely new set of embeddings for each nodal recurrent unit.

Through the timesteps/rounds of Message Passing, 
the nodes know more about their own information (features) and that of neighbouring nodes. 
This creates an even more accurate representation of the entire graph.

For further processing in higher layers of a pipeline, or simply to represent the graph,
you can take all the embeddings and sum them up together
to get vector H that represents the whole graph.

((To summarise this step, we sum together the final vector representations
of all nodal recurrent units (order-invariant,of course)
use this resulting vector as inputs to other pipelines
or to simply represent the graph.))

GNNs are fairly simple to use. In fact, implementing them involved four steps.

    1.Given a graph, we first convert the nodes to recurrent units
    and the edges to feed-forward neural networks.
    2.Then we perform Neighbourhood Aggregation (Message Passing, if that sounds better) 
    for all nodes n number of times.
    3.Then we sum over the embedding vectors of all nodes 
    to get graph representation H.
    4.Feel free to pass H into higher layers
    or use it to represent the graph’s unique properties!

Why Graph Neural Networks?
Now that we know how Graph Neural Networks work, why would we want to apply/use them?
In the case of social media graphs, GNNs are great at content recommendation. 
When a user follows other users with a similar taste in political leaning (for example), 
GNNs can be used for node classification
to predict if a certain piece of content on the site 
can be sent to the news feed of said user.

When suggesting “who to follow”, systems can take into account the industry of the user 
and provide potential connections — edge classification.

There are also great resources to learn about GDL algorithms 
and different ways to capture lots of sequential and spatial aspects from graph representations.

https://github.com/rusty1s/pytorch_geometric
https://pytorch-geometric.readthedocs.io/
https://www.dgl.ai/
http://geometricdeeplearning.com/

Geometric Learning
In the last decade, Deep Learning approaches (e.g. Convolutional Neural Networks and Recurrent Neural Networks)
allowed to achieve unprecedented performance on a broad range of problems
coming from a variety of different fields (e.g. Computer Vision and Speech Recognition).

Despite the results obtained, 
research on DL techniques has mainly focused so far on data defined on Euclidean domains (i.e. grids).

Nonetheless, in a multitude of different fields, such as: Biology, Physics, Network Science,
Recommender Systems and Computer Graphics; 
one may have to deal with data defined on non-Euclidean domains (i.e. graphs and manifolds).

The adoption of Deep Learning in these particular fields has been lagging behind until very recently,
primarily since the non-Euclidean nature of data 
makes the definition of basic operations (such as convolution) rather elusive.

Geometric Deep Learning deals in this sense 
with the extension of Deep Learning techniques to graph/manifold structured data.


--
A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction

We present a novel theory of Fermat paths of light
between a known visible scene and an unknown object not in the line of sight of a transient camera. 

These light paths either obey specular reflection or are reflected by the object’s boundary,
and hence encode the shape of the hidden object.

We prove that Fermat paths correspond to discontinuitiesin the transient measurements. 
We then derive a novel constraint 
that relates the spatial derivatives of the path lengths at these discontinuities to the surface normal.

Based on this theory, we present an algorithm, called Fermat Flow, 
to es-timate the shape of the non-line-of-sight object.

Our method allows, for the first time, accurate shape recovery of com-plex objects,
ranging from diffuse to specular, that are hid-den around the corner as well as hidden behind a diffuser.




--

Pruning 

Michela Paganini, Postdoctoral Researcher at Facebook AI, 
shares her personal experience creating a core PyTorch feature: Pruning (torch.nn.utils.prune). 

State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. 
On the contrary, biological neural networks are known to use efficient sparse connectivity. 

Identifying optimal techniques to compress models by reducing the number of parameters in them is important
in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, 
deploy lightweight models on device, and guarantee privacy with private on-device computation. 

On the research front, pruning is used to investigate the differences in learning dynamics between over-parametrized and under-parametrized networks, 
to study the role of lucky sparse subnetworks and initializations (“lottery tickets”) as a destructive neural architecture search technique, and more.




--
https://gizmodo.com/this-algorithm-might-make-facial-recognition-obsolete-1844591686

 By swapping out or distorting some of these pixels,
 the face might still be recognizable to you or me, 
 but it would register as an entirely different person to just about every popular facial recognition algo. 
 
 According to the team’s research, this “cloaking” technique managed to fool the facial recognition systems
 peddled by Microsoft, Amazon, and Google 100% of the time.

If you want to give this algo a whirl yourself, the good news is that the U. Chicago team has the Fawkes program 
freely available for download on their website. If you have a picture you want to protect from snoopers or scrapers, 
you can load them into Fawkes, which then jumbles those unseen pixels in about 40 seconds per photograph, according to the researchers. 


---

A critical analysis of self-supervision, or what we can learn from a single image
Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi

    We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels.
    We show that three different and representative methods, BiGAN, RotNet and DeepCluster,
    can learn the first few layers of a convolutional network from a single image 
    as well as using millions of images and manual labels, provided that strong data augmentation is used. 
    
    However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.





----
Efficient Nets


Image Classification with EfficientNet: -----> Better performance with computational efficiency
Anand Borad
https://medium.com/analytics-vidhya/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6

In May 2019, two engineers from Google brain team named Mingxing Tan and Quoc V. Le 
published a paper called “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. 
The core idea of publication was about strategically scaling deep neural networks 
but it also introduced a new family of neural nets, EfficientNets.

EfficientNets, as the name suggests are very much efficient computationally
and also achieved state of art result on ImageNet dataset which is 84.4% top-1 accuracy.

So, in this article, we will discuss EfficientNets in detail but first, we will talk about the core idea introduced in the paper, 
-->model scaling.

Model scaling is about scaling the existing model in terms of model depth, model width, and less popular input image resolution
to improve the performance of the model. 
Depth wise scaling is most popular amongst all, e.g. ResNet can be scaled from Resnet18 to ResNet200. 
Here ResNet10 has 18 residual blocks and can be scaled for depth to have 200 residual blocks.

ResNet200 delivers better performance than ResNet18 and thus, manually scaling works pretty well. 
But there is one problem with traditional manual scaling method, after a certain level, scaling doesn’t improve performance.
It starts to affect adversely by degrading performance.

The scaling method introduced in paper is named -->compound scaling 
and suggests that instead of scaling only one model attribute out of depth, width, and resolution; 
-->strategically scaling all three of them together delivers better results.

Compound scaling
--->Compound scaling method uses a compound co-efficient ø to scale width, depth, and resolution together.




----

Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
Debidatta Dwibedi, Ishan Misra, Martial Hebert
https://arxiv.org/abs/1708.01642

    A major impediment in rapidly deploying object detection models for instance detection
    is the lack of large annotated datasets.
    
    For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely.
    Each new environment with new instances requires expensive data collection and annotation. 
    
    In this paper, 
    >>we propose a simple approach to generate large annotated instance datasets with minimal effort. 
    
    Our key insight is that ensuring only patch-level realism 
    provides enough training signal for current object detector models. 
    We automatically `cut' object instances and `paste' them on random backgrounds. 
    
    A naive way to do this results in pixel artifacts which result in poor performance for trained models.
    We show how to make detectors ignore these artifacts during training 
    and generate data that gives competitive performance on real data. 
    
    Our method outperforms existing synthesis approaches
    and when combined with real images 
    improves relative performance by more than 21% on benchmark datasets.
    
    In a cross-domain setting, our synthetic data combined with just 10% real data 
    outperforms models trained on all real data. 


-----------

Frustratingly Simple Few-Shot Object Detection

https://arxiv.org/pdf/2003.06957.pdf

Abstract
Detecting rare objects from a few examples is an emerging problem.  
Prior works show meta-learning  is  a  promising  approach.  
But,  fine-tuning techniques have drawn scant attention. 

We find  that  fine-tuning  only  the  last  layer  of  existing detectors on rare classes is crucial 
to the few-shot object detection task.   

Such a simple approach  outperforms  the  meta-learning  methods
by roughly 2∼20 points on current benchmarks and sometimes even doubles the accurac yof the prior methods.  

However,  the high variance in the few samples 
often leads to the unreliability  of  existing  benchmarks.

We  revise the  evaluation  protocols  
by  sampling  multiple groups of training examples
to obtain stable comparisons  
and  build  new  benchmarks  based  on three datasets: PASCAL VOC, COCO and LVIS.

Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks.
The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.


--------

One-bit Supervision for Image Classification
https://arxiv.org/pdf/2009.06168.pdf

Abstract
This paper presents one-bit supervision, a novel setting of learning from incomplete annotations, in the scenario of image classification.
Instead of training a model upon the accurate label of each sample, 
our setting requires the model to query with a predicted label of each sample 
and learn from the answer whether the guess is correct. 

This provides one bit (yes or no) of information, and more importantly,
annotating each sample becomes much easier than finding the accurate label from many candidate classes.  

There are two keys to training a model upon one-bit supervision: 
improving the guess accuracy 
and making use of incorrect guesses.

For these purposes, we propose a multi-stage training paradigm 
which incorporates negative label suppression into an off-the-shelf semi-supervised learning algorithm. 
In three popular image classification benchmarks, our approach claims higher efficiency in utilizing the limited amount of annotations.


------

Relational inductive biases, deep learning, and graph networks   (combinatorial generalization)

https://arxiv.org/pdf/1806.01261.pdf

The  following  is  part  position  paper,  part  review,  and  part  unification. 
We  argue  that combinatorial generalization 
must be a top priority for AI to achieve human-like abilities,
and that structured representations and computations are key to realizing this objective. 

Just as biology uses nature and nurture cooperatively,
we reject the false choice between “hand-engineering”and “end-to-end” learning, 
and instead advocate for an approach which benefits from their complementary strengths. 

We explore how using relational inductive biases 
within deep learning architectures 
can facilitate learning about entities, relations, and rules for composing them.  

We present a new building block for the AI toolkit with a strong relational inductive bias
—the graph network—
which generalizes and extends various approaches for neural networks that operate on graphs,
and provides a straightforward interface 
for manipulating structured knowledge and producing structured behaviors.  

We discuss how graph networks can support relational reasoning and combinatorial generalization, 
laying the foundation for more sophisticated, interpretable,and flexible patterns of reasoning.  

As a companion to this paper,  we have also released anopen-source software library for building graph networks, 
with demonstrations of how to use them in practice.




---------------------

DNA computing

https://interestingengineering.com/what-is-dna-computing-how-does-it-work-and-why-its-such-a-big-deal

The reason this generated excitement was that DNA structures are cheap, relatively easy to produce, and scalable. 
There is no limit to the power that DNA computing can theoretically have 
since its power increases the more molecules you add to the equation 
and unlike silicon transistors which can perform a single logical operation at a time,
these DNA structures can theoretically perform as many calculations at a time as needed to solve a problem 
and do it all at once.

The problem however, is speed.
Even though it took moments for Adleman’s solution to the traveling salesman problem to be encoded into his DNA strands in the test tube,
it took days of filtering out bad solutions to find the optimal solution he was looking for
—after meticulous preparation for this single computation.

Still, the concept was a sound one and the potential for incredible gains 
in storage capacity
and computational speeds was obvious.
This kicked off two decades of research into how to create practical DNA computing a reality.


As demonstrated with Adleman’s paper (1994), 
the major advantage of DNA computing over classical computing—and even quantum computing to an extent—
>>>>>> is that it can perform countless calculations in parallel. 
This idea of parallel computing isn’t new and has been mimicked in classical computing for decades.


https://www.wired.com/story/finally-a-dna-computer-that-can-actually-be-reprogrammed/

But there’s a problem: The molecular circuits built so far have no flexibility at all. 
Today, using DNA to compute is “like having to build a new computer out of new hardware just to run a new piece of software,”
says computer scientist David Doty. So Doty, a professor at UC Davis, and his colleagues set out to see
what it would take to implement a DNA computer that was in fact 
> reprogrammable.

They showed it’s possible to use a simple trigger
to coax the same basic set of DNA molecules 
into implementing numerous different algorithms.

“There was >> algorithmic self-assembly before, 
but not to this degree of complexity.”

In electronic computers like the one you’re using to read this article, 
bits are the binary units of information that tell a computer what to do. 
They represent the discrete physical state of the underlying hardware, 
usually the presence or absence of an electrical current. 

These bits, or rather the electrical signals implementing them,
are passed through circuits made up of logic gates, 
which perform an operation on one or more input bits 
and produce one bit as an output.

By combining these simple building blocks over and over, 
computers are able to run remarkably sophisticated programs.
The idea behind DNA computing is to substitute 
chemical bonds
for electrical signals 
and nucleic acids 
for silicon 
to create biomolecular software. 

According to Erik Winfree, a computer scientist at Caltech and a co-author of the paper,
molecular algorithms 
leverage the natural information processing capacity baked into DNA,
but rather than letting nature take the reins, he says,
“computation controls the growth process.”


-------
6.2.2 Self-Training (Bootstrapping)

Mahendra and Schmdi

Self-training, sometimes also referred to as bootstrapping or pseudo-labels, 
is an iterative method where a deep neural network is first developed in a supervised fashion on the labelled data. 

This neural network is then used to provide (pseudo) labels to the unlabelled data, 
which can then be used in conjunction with the labelled data to train a new, more accurate neural network. 

This approach often works well and can even be repeated to get further improvements. 
There are a couple of common details in implementation — often when adding the neural network pseudo-labelled data, 
we only keep the most confidently pseudo-labelled examples. 

These pseudo-labelled examples may also be used for 
training with a different objective function 
compared to the labelled data. 

Other variants, including mean teacher [225],
temporal ensembling [119]
and the recent MixMatch [19]
also primarily use the self-training approach, but incorporate elements of consistency (see below).

There are nice open sourced implementations of these methods,
such as https://github.com/CuriousAI/mean-teacher for mean teacher 
and https://github.com/google- research/mixmatch and
https://github.com/YU1ut/MixMatch-pytorch for MixMatch.


6.2.3 Enforcing Consistency (Smoothness)

An important theme in many semi-supervised methods has been to
provide supervision on the unlabelled data 
through enforcing consistency. 

If a human was given two images A and B, 
where B was a slightly perturbed version of A (maybe blurred, maybe some pixels obscured or blacked out), 
they would give these images the same label — consistency. 

We can also apply this principle to >>> provide feedback to our neural network on the unlabelled data,
combining it with the labelled data predictions as in multitask learning (Section 5.3)
to form a semi-supervised learning algorithm. 

A popular method on enforcing consistency is virtual adversarial training [155],
which enforces consistency across carefully chosen image perturbations.

Another paper, unsupervised data augmentation [251], 
uses standard data augmentation techniques such as cutout [44]
for images and back translation for text [206] 
to perturb images and enforces consistency across them. 

[265] uses consistency constraints 
along with other semi-supervised and self-supervised techniques in its full algorithm.


------------

Learning Not to Learn: Training Deep Neural Networks with Biased Data

Byungju Kim1Hyunwoo Kim2Kyungsu Kim3Sungjin Kim3Junmo Kim1

Abstract
We  propose  a  novel  regularization  algorithm  to  train deep  neural  networks,  
in  which  data  at  training  time  isseverely biased.  

Since a neural network efficiently learns data distribution,  
a network is likely to learn the bias information to categorize input data.  
It leads to poor perfor-mance at test time, if the bias is, in fact, irrelevant to the categorization.

In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias.

Based on the idea of minimizing this mutual information, 
we propose an iterative algorithm to unlearn the bias information.  

We employ an additional network to predict the bias distribution 
and train the network adversarially against the feature embedding network. 

At the end oflearning, the bias prediction network is not able to predict the bias not because it is poorly trained,
but because the feature embedding network successfully unlearns the bias information.  
We also demonstrate quantitative and qualitative experimental results 
which show that our algorith meffectively  removes  the  bias  information  from  feature  em-bedding.

-------------
The Batch 


Bigger, Faster Transformers

Performance in language tasks rises with the size of the model — yet, as a model’s parameter count rises, 
so does the time it takes to render output. 
   >>>> New work pumps up the number of parameters without slowing down the network.
   
What’s new: William Fedus, Barret Zoph, and Noam Shazeer at Google Brain developed the Switch Transformer,
a large-scale architecture (the authors built a version comprising 1.6 trillion parameters) 
that’s nearly as fast as a much smaller model.

Key insight: The approach known as mixture-of-experts uses only a subset of a model’s parameters per input example. 
Like mixture-of-experts, Switch Transformer 
   >>> chooses which of many layers would best process a given input.

------

The Batch 

Attention for Image Generation

Attention quantifies how each part of one input affects the various parts of another. 
Researchers added a step that reverses this comparison to produce more convincing images.

What’s new: Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system GANsformer.

Key insight: Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. StyleGAN splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.

How it works: GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in FFHQ; scenes composed of cubes, cylinders, and spheres in CLEVR; pictures of bedrooms in LSUN; and urban scenes in Cityscapes.

    Given a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.
    The authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.
    Duplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.
    The discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention.

Results: GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained Inception model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to StyleGAN2’s 8.3500 FID. GANsformer also learned more efficiently than a vanilla GAN, StyleGAN, StyleGAN2, k-GAN, and SAGAN. It required a third as many training iterations to achieve equal performance.

Why it matters: Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)

We’re thinking: Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.


----------

Curiosity-driven AI

https://www.technologyreview.com/2017/05/23/151580/curiosity-may-be-vital-for-truly-smart-ai/
https://psychblog.berkeley.edu/spotlights/2019/11/05/Eliza-Kosoy/

-----------
GLOM
How to represent part-whole hierarchies in a neural network
Geoffrey Hinton

    This paper does not describe a working system.
    Instead, it presents a single idea about representation 
    which allows advances made by several different groups to be combined into an imaginary system called GLOM.
    
    The advances include transformers, neural fields, contrastive representation learning, distillation and capsules.
    
    GLOM answers the question: How can a neural network with a fixed architecture 
    parse an image into a part-whole hierarchy 
    which has a different structure for each image?
    
    The idea is simply to use islands of identical vectors to represent the nodes in the parse tree.
    If GLOM can be made to work, it should significantly improve the interpretability of the representations
    produced by transformer-like systems when applied to vision or language 


----
from paper 'inverse molecular design using machine learning'

New approaches also lie in inverse RL,
geared toward learning a reward or loss function (86). 

Research in this direction will allow for the discovery of reward functions 
associated with different materials discovery tasks.

---

from 
https://ml.berkeley.edu/blog/posts/contrastive_learning/

Contrastive learning

has proven to be one of the most promising approaches in unsupervised representation learning. 

With the evaluation metric described in the last paragraph, contrastive learning methods are able to outperform “pre-training” methods
which require labeled data. 
The idea behind contrastive learning is surprisingly simple:
the model learns to encode images in a lower dimensional space in such a way that
images that are similar semantically will be close to each other in the low dimensional space,
and at the same time far away from other images. 

For example, we would want the representation of cats to be close to other cats and far away from representation of pigs. 
In the rest of this blog post, we will elaborate on this intuition and describe how it is implemented step by step.

Zooming out, there are multiple ways we can do unsupervised representation learning. 
Some widely used ones include Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). 
Unlike contrastive learning, which only trains an encoder, 
both VAE and GAN have decoders that are capable of transforming the learned low-dimensional representation back to the original image. 
From a high level, they make sure the representation captures the relevant information by optimizing this decoder reconstruction loss. 

In contrastive learning, however, 
we directly try to regulate the distance between images within the learned representation space: 
the representation of a cat image shall be close to that of other cat images,
while also being far away from representations of pig images.


----------

from deeplearning.ai

A transformer model trained only to predict missing amino acids in a sequence 
learned important things about how amino acids form a larger structure. 

Interpreting self-attention values reveals not only how a model works 
but also how nature works. 



--------

from the batch

Crawl the Web, Absorb the Bias

The emerging generation of trillion-parameter models needs datasets of billions of examples, but the most readily available source of examples on that scale — the web — is polluted with bias and antisocial expressions. A new study examines the issue.

What’s new: Abeba Birhane and colleagues at University College Dublin and University of Edinburgh audited the LAION-400M dataset, which was released in September. It comprises data scraped from the open web, from which inaccurate entries were removed by a state-of-the-art model for matching images to text. The automated curation left plenty of worrisome examples among the remaining 400 million examples — including stereotypes, racial slurs, and sexual violence — raising concerns that models trained on LAION-400M would inherit its shortcomings.

Key insight: The compilers of LAION-400M paired images and text drawn from Common Crawl, a large repository of web data. To filter out low-quality pairs, they used CLIP to score the correspondence between them and discarded those with the lowest scores. But CLIP itself is trained on a massive trove of web data. Thus it’s bound to find a high correspondence between words and pictures that are frequently associated with one another on the web, even if the associations are spurious or otherwise undesirable.

NSFT (not safe for training): The authors entered text queries into LAION-400M’s search function, which returned matching images. 

    In response to queries about women, for instance “latina,” “aunty,” and “nun,” the search engine returned a high percentage of pornography and depictions of sexual violence. Similarly, some non-gendered queries including “Korean” and “Indian” returned sexually-explicit images of women.
    Other queries returned biased results. For example, “CEO” returned images of men but not women. “Terrorist” returned images of Middle Eastern men but not people wearing Ku Klux Klan outfits.
    Examining CLIP, the authors found that the 0.3 cosine similarity threshold didn’t weed out image/text pairs that expressed stereotypes, sexism, or racism. For instance, CLIP gave a passing score to a female astronaut’s portrait accompanied by the words, “this is a photograph of a smiling housewife in an orange jumpsuit with the American flag.”

Behind the news: The LAION-400M team, a loosely knit collective led by Christoph Schuhmann at University of Vienna, aims to re-create Google’s Wikipedia-based Image Text dataset and ultimately use it to train open-source analogs of OpenAI’s CLIP and DALL·E. The group was inspired by EleutherAI’s community effort to build an open source version of GPT-3.

Why it matters: It’s enormously expensive to manually clean a dataset that spans hundreds of millions of examples. Automated curation has been viewed as a way to ensure that immense datasets contain high-quality data. This study reveals serious flaws in that approach.

We’re thinking: Researchers have retracted or amended several widely used datasets to address issues of biased and harmful data. Yet, as the demand for data rises, there’s no ready solution to this problem. Audits like this make an important contribution, and the community — including large corporations that produce proprietary systems — would do well to take them seriously.
